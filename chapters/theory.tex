This chapter presents relevant theoretical background for the concepts and methodologies used in this research.

\section{Time Series Forecasting}
A time series is a set of chronologically arranged observations, generally under the assumption that time is a discrete variable \cite{kirchgassner_introduction_2012}. In a time series with $T$ real value samples $x_1,\dots,x_T$, the value at time $i$ is represented by $x_i$ ($1 \leq i \leq T$). The problem of time series forecasting can be defined as predicting the values of $x_{w+1},\dots,x_{w+h}$, where $w$ is the historical window, i.e., the number of values considered in order to produce the prediction, and $h$ is the prediction horizon, i.e, the number of future values to be predicted. Prediction happens given the previous samples $x_1,\dots,x_w$ ($w+h \leq T$), with the goal of minimizing the error between the predicted value $\hat{x}_{w+i}$ and the actual value $x_{w+i}$ ($1 \leq i \leq h$) \cite{divina_stacking_2018}.

A time series may be univariate or multivariate, where univariate refers to a series with a single observation recorded over time, and multivariate refers to a series with a group of variables with interactions.
%In order to draw inferences from a time series, a model representing the data needs to be set up
Analysis of time series often begins with obtaining the underlying patterns of the observed data. The next step is fitting a model to represent the data for future predictions, which can be a complex task. The analysis of the time series is usually performed by decomposing it into three components \cite{deb_review_2017,brockwell_introduction_2002}:
\begin{enumerate}
    \item \emph{Trend} \hyp{} The general movement of the variable during the observation period. Trend does not take irregularities and seasonality into account.
    \item \emph{Seasonality} \hyp{} The periodic fluctuation of the variable. It includes stable effects, along with time, magnitude, and direction.
    \item \emph{Residual} \hyp{} The remaining, largely unexplainable part of the time series. In some instances, this can be high enough to mask the trend and seasonality.
\end{enumerate}

Both linear and nonlinear techniques may be used for solving time series problems. In linear methods, the idea is that strong correlations in the data allow for linear combinations to determine the next observation. However, the random component of the time series may prevent precise predictions. Nonlinear methods are applied in the machine learning domain to forecast future values based on a model describing the data. Machine learning solutions to forecasting problems have gained popularity, due to the fact that they are suitable both for linear and nonlinear problems \cite{divina_stacking_2018}. Time series analysis has been extensively applied to the problem of energy consumption forecasting, especially since real-time monitoring of buildings has become more common, and the availability of recorded data has increased \cite{deb_review_2017}.

The scope of energy consumption forecasting problems can generally be divided into three categories; short-term forecasting (1 hour to 1 week), medium-term forecasting (1 month to 1 year), and long-term forecasting (1 year and above) \cite{mat_daut_building_2017}. [maybe move this to introduction for context]

\section{Ensemble Learning}

Ensemble learning is a family of methods that combine several machine learning algorithms to make decisions. It can be seen as the ML interpretation of ``wisdom of the crowd'', that is, the idea that aggregating the opinions of several individuals is better than selecting the opinion of one individual. Multiple weak learners make predictive results that are fused together via voting mechanisms, in order to achieve better performance than from any single algorithm. Any type of machine learning algorithm, e.g. decision tree, neural network, or regression model, can be used as an ensemble learner \cite{sagi_ensemble_2018,dong_survey_2020}.

According to Dietterich \cite{dietterich_ensemble_2000}, there are three reasons why ensemble learning methods achieve good performance in machine learning. The first reason is statistical. Models can be seen as searching for the best hypothesis within a hypothesis space $H$. When data is limited, which it often is, the models can find several hypotheses in $H$ that give the same accuracy on the training data. This makes it hard to choose between hypotheses, since it is unknown which one will generalize better for unseen data. Ensemble learning can help with avoiding this problem of overfitting. Secondly, ensemble learning provides computational benefits. Many models function via performing some type of local search, which may be prone to getting stuck in a local optima. A better approximation of the unknown true function may be obtained from an ensemble constructed by the local search started from many different points. The third reason is representational. In most situations, the unknown true function may not be included in $H$. Combining several hypotheses from $H$, the space of representable functions can be expanded to possibly include the true function.

% kan vara värt att nämna decision trees här
Three common strategies for constructing ensembles are bagging, boosting, and stacking \cite{moon_advancing_2024}. Bagging, or bootstrap aggregating, proposed by Breiman \cite{breiman_bagging_1996} is a method for obtaining an aggregated prediction by building multiple versions of a predictor. Bootstrapping is used to generate multiple samples of data from the original dataset, which are then used to independently train the different predictors. Majority voting is used to aggregate the final prediction \cite{sagi_ensemble_2018}. The main objective of the bagging technique is to reduce the variance of a prediction model. A common example of the bagging technique is the random forest (RF) algorithm \cite{moon_advancing_2024,ribeiro_ensemble_2020}.

In boosting, several models with individually weak predictions are aggregated to obtain a model with high accuracy. Models are trained sequentially to correct the errors of previous models. The main objective with this technique is reducing model bias \cite{moon_advancing_2024,ribeiro_ensemble_2020,schapire_strength_1990}. Two examples of boosting algorithms are Gradient Boosting Machines (GBM), which uses gradient descent to minimize the loss function, and XGBoost, which is an extension of GBM focused on resource optimization and prevention of overfitting \cite{ribeiro_ensemble_2020}.

Stacking utilizes different learning algorithms called base learners to make initial predictions. These outputs are then combined via a meta-learner to make the ultimate predictions. The goal is to capture a wider range of patterns using the strength of different models, while minimizing the generalization errors \cite{moon_advancing_2024,ribeiro_ensemble_2020,divina_stacking_2018}. Accuracy improvements mainly happen when there is diversity among the individual models, since differences in generalization principles generally lead to different results \cite{ribeiro_ensemble_2020}.

\section{Explainable Artificial Intelligence}

In recent years, artificial intelligence has been widely adopted in society, and humans increasingly rely on AI as part of decision-making. Research in the field of AI has for many years focused on improving the predictive accuracy. However, the black-box nature of many AI systems creates a lack of transparency, and explaining to users how the AI systems work is highly problematic. Pressure from society, ethics, and legislation has created a demand of a new type of AI that is interpretable to users and can explain its inner functions. To address these issues, explainable AI methods that elucidate the decision-making process have been developed \cite{minh_explainable_2022,angelov_explainable_2021}.

XAI methods can be applied at different levels of the AI modelling. Firstly, explainability can be applied pre-modelling through means of data pre-processing and data analysis. This approach gives insights into the datasets used for training the ML models, and it can expose imbalances in the data. Secondly, the model itself can be inherently interpretable. This refers to models that have a simple structure, such as linear regression and k-Nearest Neighbors. Models like these are understandable by design, and users are able to interpret the model based on the model summary or parameters. Thirdly, explainability can be applied post-modelling to deal with the black-box problem of more complex ML models. Post-modelling methods include textual and visual justifications, simplification of the model, and feature relevance. These methods can explain how an algorithm performs during training, and how predictions are generated given a certain input \cite{minh_explainable_2022}.

Ensemble models are based on the aggregation of several models, which makes it more complex to interpret the results. Post-modeling XAI methods such as simplification and feature relevance can be implemented to explain the ensemble \cite{minh_explainable_2022}.

[Feature importance]

\subsection{SHAP}

SHapley Additive exPlanations (SHAP) is a popular method for model explainability. It is model-agnostic, meaning that it can be used for any ML model. The SHAP method has its foundation in game theory, where the contribution of each player to the total payoff is calculated. In the case of machine learning, the players are represented by features (variables) and the payoff is the model output. To calculate the contribution score for each feature, all different combinations of features (i.e., coalitions in game theory) are evaluated. SHAP can be used to explain model outputs both locally and globally, i.e., both for specific instances and for all instances \cite{salih_perspective_2025}.

When using SHAP for model explanations, it is important that the users are aware of some important considerations of the method. Firstly, the SHAP method is model-dependent, meaning it can produce different explanations for the same task when different models are applied. Secondly, the explainability scores assigned to features do not represent the weights of the features. Rather, the explainability lies in the ranking of the features, meaning that feature importance is deduced from the ordering. Thirdly, it is assumed that the features are independent of each other. Therefore, some features that are correlated with other features may receive low scores even though they contribute significantly to the output. This is because the correlated features have already accounted for the impact of the low-scoring feature. [There is another point about bias in this paper] Because of these points, it is crucial to present SHAP results in an informative way by including corresponding output plots and assumptions behind the method \cite{salih_perspective_2025}.

[is it okay to use the same reference for all this?]

[write mathematical notation of SHAP \cite{maarif_energy_2023}]