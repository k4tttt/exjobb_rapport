Research in the field of electricity consumption forecasting has established many methods for producing forecasts. These methods can roughly be divided into two categories; conventional methods and AI-based methods. Conventional methods such as time series analysis and regression methods have previously been widely applied to solve the problem of electricity consumption forecasting. Nowadays, AI-based methods are more popular due to their strength in solving nonlinear problems, something that the conventional methods struggle with \cite{mat_daut_building_2017}.

Two comprehensive reviews on electricity consumption forecasting have been identified:

\begin{itemize}
    \item An extensive review and comparison of both statistical and ML/DL techniques for forecasting is found in \cite{deb_review_2017}. They also review combinations of different techniques, i.e. hybrid models. Claims that ANN has more advantages than statistical models, and has better performance for nonlinear problems. Highlights that hybrid models can be beneficial to capture complexities in building energy and operational data.
    \item Another review of statistical, AI, and hybrid methods for forecasting is found in \cite{mat_daut_building_2017}. They also highlight the strength in AI models for dealing with nonlinear patterns. Claims that hybrids between AI and Swarm Intelligence (SI) methods show potential for increased accuracy. Provides a clear overview of different studies regarding prediction time intervals, included features, building types etc.
\end{itemize}

\noindent
Papers where experiments have been performed:

\begin{itemize}
    \item Support Vector Machine (SVM) for forecasting energy consumption: \cite{dong_applying_2005}.
    \item Monthly electricity consumption forecasting based on decomposition methods and ARIMA: \cite{sun_monthly_2019}.
    \item Forecasting cooling energy using ANN (for three university buildings, weekly \slash monthly): \cite{deb_forecasting_2016}.
    \item SVR and fruit fly optimization with seasonal indexing to address the fact that electricity consumption has a seasonal component: \cite{cao_support_2016}. Results show that the proposed model is a reliable forecasting tool.
\end{itemize}

\noindent
Papers on ensemble learning for energy consumption forecasting:

\begin{itemize}
    \item Forecasting high voltage consumers' monthly electricity consumption using an ensemble learning approach based on LSTM, GRU, TCN: \cite{hadjout_electricity_2022}.
    \item Ensemble learning for short-term electricity consumption forecasting: \cite{divina_stacking_2018}.
    \item Ensemble learning for short-term electricity consumption forecasting of office buildings: \cite{pinto_ensemble_2021}.
    \item Ensemble learning for annual electricity consumption forecasting: \cite{chen_novel_2018}. Predictions are made using numerous different variables, but not historical consumption. Identifies that there is a lack of household-level data for the task of prediction based on historical consumption data (time series).
\end{itemize}

\noindent
Papers specifically on XAI and energy consumption forecasting:

\begin{itemize}
    \item Forecasted hourly energy consumption for the steel sector using three different LSTM models \cite{maarif_energy_2023}. Used SHAP to interpret the decision-making, and found that leading current reactive power and the number of seconds from midnight contributed significantly to the model output.
    \item Ensemble learning for electricity consumption forecasting. Evaluated several decision tree-based ensemble learning techniques using SHAP \cite{moon_advancing_2024}. Found that ensemble learning models outperform deep learning models. Also found that temperature-humidity index and wind chill temperature has a greater impact on short-term forecasts than more traditional parameters such as temperature. Released the code at \url{https://github.com/sodayeong/PLOS-ONE_Github}. Predictions are made in a daily scope. Data is from university dormitory buildings in South Korea (i.e. probably very low diversity in buildings) with hourly measurements.
    \item Predicted electricity consumption for residential buildings based on hourly data with information about consumption for different household areas (such as kitchen and appliances) \cite{janjua_enhancing_2024}. Used LSTM as prediction model, and LIME and SHAP to provide comprehensible explanations of the predictions.
    \item Proposed a methodology for selecting input variables for energy consumption prediction using XAI (SHAP) \cite{sim_explainable_2022}. Used Extreme Gradient Boosting (XGBoost), Support Vector Regression (SVR), Light Gradient Boosting Model (LightGBM), and LSTM for prediction. Found that variables with strong impact on the forecast include year, hour, energy consumption difference, temperature, and surface-temperature.
\end{itemize}

Overfitting can be a problem with ML/DL models

%\section{Conventional Approaches}
%probably not relevant since i wont be using it

\section{Machine Learning Approaches}

\section{Ensemble Learning Approaches}

 [These parts may be more suitable in theory].

Ensemble learning is a family of methods that combine several machine learning algorithms to make decisions. It can be seen as the ML interpretation of ``wisdom of the crowd'', that is, the idea that aggregating the opinions of several individuals is better than selecting the opinion of one individual. Multiple weak learners make predictive results that are fused together via voting mechanisms, in order to achieve better performance than from any single algorithm. Any type of machine learning algorithm, e.g. decision tree, neural network, or regression model, can be used as an ensemble learner \cite{sagi_ensemble_2018,dong_survey_2020}.

According to Dietterich \cite{dietterich_ensemble_2000}, there are three reasons why ensemble learning methods achieve good performance in machine learning. The first reason is statistical. Models can be seen as searching for the best hypothesis within a hypothesis space $H$. When data is limited, which it often is, the models can find several hypotheses in $H$ that give the same accuracy on the training data. This makes it hard to choose between hypotheses, since it is unknown which one will generalize better for unseen data. Ensemble learning can help with avoiding this problem of overfitting. Secondly, ensemble learning provides computational benefits. Many models function via performing some type of local search, which may be prone to getting stuck in a local optima. A better approximation of the unknown true function may be obtained from an ensemble constructed by the local search started from many different points. The third reason is representational. In most situations, the unknown true function may not be included in $H$. Combining several hypotheses from $H$, the space of representable functions can be expanded to possibly include the true function.

% kan vara värt att nämna decision trees här
Three common strategies for constructing ensembles are bagging, boosting, and stacking \cite{moon_advancing_2024}. Bagging, or bootstrap aggregating, proposed by Breiman \cite{breiman_bagging_1996} is a method for obtaining an aggregated prediction by building multiple versions of a predictor. Bootstrapping is used to generate multiple samples of data from the original dataset, which are then used to independently train the different predictors. Majority voting is used to aggregate the final prediction \cite{sagi_ensemble_2018}. The main objective of the bagging technique is to reduce the variance of a prediction model. A common example of the bagging technique is the random forest (RF) algorithm \cite{moon_advancing_2024,ribeiro_ensemble_2020}.

In boosting, several models with individually weak predictions are aggregated to obtain a model with high accuracy. Models are trained sequentially to correct the errors of previous models. The main objective with this technique is reducing model bias \cite{moon_advancing_2024,ribeiro_ensemble_2020,schapire_strength_1990}. Two examples of boosting algorithms are Gradient Boosting Machines (GBM), which uses gradient descent to minimize the loss function, and XGBoost, which is an extension of GBM focused on resource optimization and prevention of overfitting \cite{ribeiro_ensemble_2020}.

Stacking utilizes different learning algorithms to make outputs, and then a meta-learner to combine the outputs for the ultimate predictions \cite{moon_advancing_2024,divina_stacking_2018}. 

Ensemble learning has previously been applied for the task of electricity consumption forecasting, mostly in the short-term scope. Divina et al. \cite{divina_stacking_2018} proposed a stacking ensemble learning approach for short-term  forecasting of Spain's general electricity consumption. They used three basic learning methods (regression trees based on Evolutionary Algorithms, Artificial Neural Networks, and Random Forests) and a top level method that produced final predictions [skriv om detta stycke]